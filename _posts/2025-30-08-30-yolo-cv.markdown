---
layout: learning
title:  "YOLO"
subtitle: "CV using Ultralytics YOLO models"
summary: "Proof-of-Concept image/video processing using YOLO models for pose detection, object detection, classification, and segment extraction."
date:   2025-08-30 21:03:01 +0700
categories: ["edu"]
image: "../media/images/cv.png"
tags: ["cv", "detection", "segmentation"]
author: "Karie Moorman"
page_type: pages
---


<h3 align='center'>Table of Contents</h3>
<div class='tbl'>
<div class='centered-list'>
<ul>
<li><a href='#intro'>Overview</a></li>
<li><a href='#model'>Models</a></li>
<li><a href='#image-det'>Image Detection</a></li>
<li><a href='#image-seg'>Image Segmentation</a></li>
<li><a href='#video-det'>Video Detection</a></li>
<li><a href='#train'>Custom Models</a></li>
</ul>
</div>
</div>

--- 

<div class='page-conf'>
<h3 id='intro' align='center'>Overview</h3>
<p>Computer Vision (CV) is a field of artificial intelligence that enables machines to interpret and understand visual information from the world, such as images and videos. By mimicking human vision, computer vision algorithms can detect objects, recognize faces, track movements, analyze scenes, and perform tasks like pose detection and segmentation.</p>

<p align='center'>Github: <a href='https://github.com/kariemoorman/didactic-diy/blob/main/tutorials/vision/yolo-cv.py' target='_blank'>YOLO-CV</a></p>

</div>

---

<h3 id='model' align='center'>Models</h3>

<h4>YOLO</h4>

YOLO models are real-time object detection models known for their speed and accuracy, often converted to Core ML for on-device detection tasks. For this project we use YOLOv5m for object detection tasks.

The latest model version is [YOLO11](https://docs.ultralytics.com/models/yolo11/#key-features). 

5 base models are available depending on the computer vision task:

- YOLO11: Detection
- YOLO11-obb: Oriented-Detection
- YOLO11-seg: Segmentation
- YOLO11-cls: Classification
- YOLO11-post: Pose-Estimation

This example will make use of YOLO11, YOLO11-post, and YOLO11-seg models.


---

<h3 id='image-det' align='center'>Image Detection</h3>

Off-the-shelf YOLO models perform well at pose detection and object detection across [80 predefined classes](https://docs.ultralytics.com/datasets/detect/). 


Required Libraries: 

```bash
ultralytics
opencv-python
```


Basic Python Script:

```python

import os
import cv2
from ultralytics import YOLO


def process_image(image_filepath: str, model_type: str):
    if not os.path.isfile(image_filepath):
        raise ValueError(f"Invalid file path: {image_filepath}. The file does not exist or is not a valid file.")
    
    model_dict = {
        'pose': "yolo11m-pose.pt",
        'obj': "yolo11m.pt",
        'seg': "yolo11m-seg.pt"
    }

    model_name = model_dict.get(model_type)

    if model_name is None:
        raise ValueError(f"Invalid model_type: {model_type}. Choose from 'pose', 'obj', or 'seg'.")
    
    model = YOLO(model_name)

    filepath = os.path.splitext(os.path.basename(image_filepath))[0]
    results = model(image_filepath)
    for result in results:
        annotated_image = result.plot()
    cv2.imwrite(f"{filepath}_{model_type}.png", annotated_image)

```

<br>

<h4>Example</h4>


For the example we'll use the following input image:

input_image='bus.jpg'

<p align='center'><img src='/media/images/yolo/bus.jpg' alt='yolo-bus' width='50%'></p>

<br>

Python Commands:

```python

process_image(input_image, 'obj')

process_image(input_image, 'pose')

```

<br>

Output Detection: 

<div style="display: flex; flex-wrap: wrap; justify-content: space-evenly; align-items: center; max-width: 600px; gap: 10px; margin: 0 auto;">
  <img src="/media/images/yolo/bus_obj.png" alt="OBJ" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_pose.png" alt="POSE" style="flex: 1 1 45%; max-width: 45%;">
</div>

<br>

Using object and pose detection, we can understand features represented in an image such as class location and density, orientation and directionality, in an automated and scalable fashion.

<br>


---

<h3 id='image-seg' align='center'>Image Segmentation</h3>

Using YOLO11-seg model, individual objects segmented can be extracted and saved as independent images. 


Required Libraries: 

```bash
numpy
pillow
ultralytics
opencv-python
```

Basic Python Script:

```python

import os
import cv2
import numpy as np
from PIL import Image, ImageDraw
from ultralytics import YOLO

def extract_image_objs(image_filepath: str, background_type: str ='transparent'):
    if not os.path.isfile(image_filepath):
        raise ValueError(f"Invalid file path: {image_filepath}. The file does not exist or is not a valid file.")
    
    if background_type not in ['black', 'transparent']:
        raise ValueError(f"Invalid background_type: {background_type}. Please select 'transparent' or 'black'.")

    filepath = os.path.splitext(os.path.basename(image_filepath))[0]

    model = YOLO("yolo11m-seg.pt")

    results = model.predict(image_filepath)
    result = results[0]
    masks = result.masks
    class_names = result.names
    img = cv2.imread(image_filepath)
    if masks is not None:
        for j, mask in enumerate(masks):
            mask_array = mask.data[0].numpy()
            mask_resized = cv2.resize(mask_array, (img.shape[1], img.shape[0]))
            binary_mask = np.uint8(mask_resized * 255)
            if background_type == 'black':
                mask_img = np.zeros_like(img)
                mask_img[binary_mask == 255] = img[binary_mask == 255]
            elif background_type == 'transparent':
                mask_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
                mask_img[:, :, 3] = 0
                mask_img[binary_mask == 255, :3] = img[binary_mask == 255]
                mask_img[binary_mask == 255, 3] = 255
            class_idx = result.boxes.cls[j]
            class_name = class_names[int(class_idx)]
            output_filename = f'{filepath}_{class_name}_{j}_{background_type}.png'
            cv2.imwrite(output_filename, mask_img)
            print(f"Saved extracted object {j} as {output_filename}")
    else:
        print(f"No masks found for result")

```

<br>

The python function above (`extract_image_objs`) processes an image and extracts object segment `masks` using a YOLO model. 

The YOLO model generates predicted segmentation `masks` for the objects detected in the image. These `masks` are binary arrays that highlight the regions of the image corresponding to each detected object, allowing for precise object segmentation and extraction.

For each mask detected, the function resizes the mask to match the input image's dimensions. It then defines a binary mask (`binary_mask = np.uint8(mask_resized * 255)`) which converts the mask to an unsigned 8-bit integer format (standard for image data). After resizing, the mask may still have floating-point values between 0 and 1, so multiplying by 255 and casting to `np.uint8` scales the mask values to the full 8-bit range (0-255). The result is a binary mask where non-zero values (i.e., the detected object) are 255, and the rest are 0.

Depending on specified `background_type`, the function either creates a mask with a `black` or `transparent` background. 

For `black` background, a new image is created (`mask_img = np.zeros_like(img)`) initialized to zeros (black), with the same dimensions and data type as the original image. The pixels from the original image are then copied into the new image, but only where `binary_mask` is 255 (`mask_img[binary_mask == 255] = img[binary_mask == 255]`). This effectively isolates the object: the object pixels of the original image are preserved, while the remaining pixels remain black.

For `transparent` background adds an alpha channel. The original image is converted from BGR (3-channel) format to BGRA (4-channel) format using OpenCV (`mask_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)`). The additional channel, A (alpha), represents the transparency of the image. The alpha channel is set to 0, making the image fully transparent by default (`mask_img[:, :, 3] = 0`). For every pixel where `binary_mask` is 255 (i.e., object detected), the color channels (RGB) from the original image are copied (`mask_img[binary_mask == 255, :3] = img[binary_mask == 255]`). For the same pixels where the mask is 255, the alpha channel is set to 255, making those pixels fully visible (`mask_img[binary_mask == 255, 3] = 255`). The result is an image where the detected object is visible, and the background is transparent.

The object mask is saved as a PNG file with a name indicating the class name, mask index, and background type.

<br>

<h4>Example</h4>

For the example we'll use the same input image:

input_image='bus.jpg'

<p align='center'><img src='/media/images/yolo/bus.jpg' alt='yolo-bus' width='50%'/></p>


Python Commands: 

```python 
extract_image_objs(input_image, 'black')

extract_image_objs(input_image, 'transparent')
```

<br>

Output Segments: 

<div style="display: flex; flex-wrap: wrap; justify-content: space-evenly; align-items: center; max-width: 600px; gap: 10px; margin: 0 auto;">
  <img src="/media/images/yolo/bus_bus_0_black.png" alt="Segment1B" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_bus_0_transparent.png" alt="Segment1T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_1_black.png" alt="Segment2B" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_1_transparent.png" alt="Segment2T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_2_black.png" alt="Segment3T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_2_transparent.png" alt="Segment3T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_3_black.png" alt="Segment4T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_3_transparent.png" alt="Segment4T" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_4_black.png" alt="Segment5B" style="flex: 1 1 45%; max-width: 45%;">
  <img src="/media/images/yolo/bus_person_4_transparent.png" alt="Segment5T" style="flex: 1 1 45%; max-width: 45%;">
</div>

<br>

Using these mask segments, we can perform a wide variety of interesting qualitative and quantitative analyses, including figure-ground organization, occlusion analysis, model attention and saliency, position tracking, class frequency and distribution.

These masks can also power higher-level analyses:
- Behavioral analysis (e.g., pedestrian intent, group dynamics)
- Anomaly detection (e.g., strange objects appearing in restricted zones)
- Simulation input (e.g., for robotics or autonomous systems)
- Data augmentation (e.g., cut-paste recombination of objects)

<br>

---


<h3 id='video-det' align='center'>Video Detection</h3>

Similar to image processing, video-based pose and object detection analyzes each frame individually, annotates it, and compiles the annotated frames into a video format.


Required Libraries: 

```bash
ultralytics
opencv-python
```


Basic Python Script: 

```python

import os
import cv2
from ultralytics import YOLO

def process_video(video_filepath: str):
    if not os.path.isfile(video_filepath):
        raise ValueError(f"Invalid file path: {video_filepath}. The file does not exist or is not a valid file.")

    filepath = os.path.splitext(os.path.basename(video_filepath))[0]
    model = YOLO("yolo11n-pose.pt") 

    cap = cv2.VideoCapture(video_filepath)  

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_filepath = f"{filepath}_pose.mp4"
    out = cv2.VideoWriter(output_filepath, 
                        cv2.VideoWriter_fourcc(*'mp4v'), fps, 
                        (frame_width, frame_height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        results = model(frame, show=False, verbose=False)

        if results:
            result = results[0]
            annotated_frame = result.plot()
            people_count = sum(1 for cls in result.boxes.cls if cls == 0) 
            cv2.putText(annotated_frame, f"Person Count: {people_count}", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
            out.write(annotated_frame)

    cap.release()
    out.release()
    cv2.destroyAllWindows()

```

<br>

<h4>Example</h4>

For the example we'll use the same input image:

input_video='a_man_running.mp4'

<p align='center'><video width="640" height="360" controls>
  <source src="/media/images/yolo/a_man_running.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video></p>

<br>

Python Command:

```python

process_video(input_video)

```

<br>

To add original audio to output video:

```python

ffmpeg -i a_man_running.mp4 -i a_man_running_pose.mp4 -c:v copy -c:a aac -map 0:v:0 -map 1:a:0 -shortest a_man_running_pose_with_audio.mp4

```
 


<br>

Output Detection: 


<p align='center'><img src='https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExaXN4OHU3dGZiZ2ZpcTdzbHl1bWZ4MWNxMWVob2xxaHl4Zjk0cmg5YyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/nUHnnxPCbB7mLYyx1t/giphy.gif' width="640" height="360"></p>


<br> 

Object tracking applications include: 
- Trajectory Analysis: Understand how objects move across time (e.g., person walking, vehicle turning)
- Behavior Prediction: Analyze object paths to anticipate future actions (e.g., crossing the street)
- Flow Estimation: Measuring entry/exit rates in zones (e.g., store entrances, highways)
- Object Interaction Detection: Detecting when objects come close, collide, or interact.
- Re-identification: Tracking the same object across multiple cameras or scenes.

Pose detection applications include:
- Human Activity Recognition: Classifying actions like walking, running, sitting, and waving (e.g., for gesture control, fitness apps, surveillance)
- Motion Quality Analysis: Measuring precision, range of motion, or joint angles (e.g., physical therapy, sports coaching, ergonomics)
- Fall Detection: Detect abnormal poses like sudden collapse (e.g., for elderly care, hospital monitoring)
- Gait Analysis: Analyzing walking patterns (e.g., for medical diagnostics or identity verification)
- Augmented Reality (AR): Overlay graphics that respond to user pose (e.g., filters, motion-driven avatars)

Application combining object tracking with pose detection include:
- Track individuals and what they’re doing:
  - Follow Person A across frames and know they are "walking" → "waving" → "falling"
  - Animation/Motion Capture: Capture real human motion for games or films

- Analyze interactions between people or objects:
  - Detect group behavior, mimicry, or unsafe proximity (e.g., in factories)
  - Sports analytics: Who passed the ball, when, and how was their form?

- Build detailed behavior models:
  - Pose sequences over time = action signature → train models to classify or predict behavior
  - Surveillance: Detect suspicious motion patterns, loitering, or aggression
  - Robotics: Real-time human-robot collaboration with gesture-based commands


<br> 

---


<h3 id='train' align='center'>Custom Models</h3>

Training a YOLO model on custom dataset is a relatively simple process, requiring 3 parts:
- a pre-trained model
- a dataset, split into train/val/test
- a config file

For the pretrained model, I'll use `yolo11n.pt`.

For a dataset, I'll train the model to recognize trees. I have roughly 2500 images split 70/15/15:
- Training set: 1750 images
- Validation set: 375 images
- Test set: 375 images

I use the following directory structure: 

```
├── tree_dataset
│   ├── test
│   │   ├── images
│   │   └── labels
│   ├── train
│   │   ├── images
│   │   └── labels
│   └── val
│       ├── images
│       └── labels
```

The model is trained on the training set, evaluated on the validation set, with model performance confirmed on the test set.

For a config file, I'll specify the following parameters, and save as `data.yaml`: 

```
train: tree_dataset/train/images
val: tree_dataset/val/images
test: tree_dataset/test/images

nc: 1
names: ['Tree']
```

I only have one class, as I am detecting "Tree" at the highest level, with no delineation between species.

<br>

<h4>Model Training</h4>

Required Libraries:

```
ultralytics
```

Basic Python Script:

```
data_yaml = 'data.yaml'
base_model = YOLO('yolo11n.pt') 

results = base_model.train(data=data_yaml, epochs=100, batch=16, imgsz=640, device='mps')

```

To enable training on Apple silicon, I use `mps`.

If model training is interrupted (e.g., the latest MacOS suddenly crashes...), it can be resumed using the following specification: 

```
train_model = YOLO("runs/detect/train/weights/last.pt")
results = train_model.train(resume=True)
```

Model performance on predicting trees in an unseen image: 

<p align='center'><img src='/media/images/yolo/predict_trees.jpg' alt='tree-predict' width='60%'></p>

Preliminary inspection of the image provides a few insights:  
- trees are detected
- bounding boxes are a little big, and include multiple trees
- not all trees present in the image are detected by the model

This points to a potential gap in labeling of training data. It may be helpful to create/improve training data that provides the level of detail we want the model to learn. It could also mean the model needs more epochs to learn to delineate between trees.

<br>

<h4>Model Evaluation</h4>

Once model training is complete, it is helpful to evaluate the model across epochs to inspect model performance, model convergence, and indications of over/underfitting. 

Ideally the model shows a steady decrease in the loss (e.g., training loss, validation loss), indicating that the model is learning from the data and converging toward a solution. If the loss starts to plateau (i.e., stops decreasing or decreases very slowly), it could suggest that the model has already learned most of the patterns and might need more data, different hyperparameters, or different techniques like regularization.

If training loss is oscillating or not decreasing smoothly, this could indicate the model is making too large of updates to its parameters (i.e., high learning rate). If the training loss is decreasing very slowly, and it appears like the model is taking too long to learn or converge, this could be due to low learning rate.

If training loss is decreasing while validation loss increases, this is a sign of overfitting. However, if training loss and validation loss both decrease (or stabilize) in tandem, this suggests that the model is generalizing well and learning useful features that apply to both the training and validation data. 

If the model shows low accuracy and high loss during training, this could indicate the model is not able to fit the training data well. Similarly, if the model shows low accuracy and high loss during validation, this suggests the model is unable to generalize the unseen data (i.e., underfitting).

<br>

<p align='center'><img src='/media/images/yolo/results.png' alt='yolo-train-metric'></p>

<br>

In this case, the model appears to be converging, and there is no clear indication of model over/underfitting, however recall does show signs of instability.


To evaluate model performance on validation dataset, one can use the following specification:

```
tree_model = YOLO("runs/detect/train/weights/best.pt")
metrics = tree_model.val()
```

The output helps determine if and how to tune the hyperparameters (e.g., learning rate, batch size, number of epochs, loss function, optimizer, regularization) (see [train settings](https://docs.ultralytics.com/modes/train/#train-settings)).

Validation Results: 

- `'metrics/precision(B)': 0.896167804338945`
  - Precision measures how many of the detected positive predictions (bounding boxes) are actually correct (i.e., true positives). A precision of 0.896 means that about 89.6% of the bounding boxes predicted by the model are correct. 
  - A high precision indicates that the model is making fewer false positive detections.
- `'metrics/recall(B)': 0.813953488372093`
  - Recall measures how many of the actual positive objects (ground truth bounding boxes) were correctly detected by the model. 
  - A recall of 0.814 means that the model detected 81.4% of the true objects in the dataset.
- `'metrics/mAP50(B)': 0.908113313995095`
  - mAP50 (mean Average Precision at IoU=0.5) is a standard metric in object detection that averages the precision across all classes at an Intersection over Union (IoU) threshold of 0.5. 
  - An mAP50 of 90.8% indicates that the model is making very accurate predictions, and the detections are very close to the ground truth boxes.
- `'metrics/mAP50-95(B)': 0.6970095080871962`
  - mAP50-95 is the average of mAP scores computed at multiple IoU thresholds (from 0.5 to 0.95 in increments of 0.05). This gives a more nuanced evaluation of the model’s performance across different levels of localization precision. 
  - An mAP50-95 score of 69.7% indicates that the model’s precision decreases as the IoU threshold increases.
- `'fitness': 0.718119888677986`
  - Fitness score is an aggregated or weighted metric that combines several performance measures (like precision, recall, and mAP) into one overall value. 
  - A fitness score of 0.718 indicates a relatively good overall performance, but could benefit from further fine-tuning.


While the model performs reasonably well, validation metrics indicate hyperparameter tuning can improve model performance in detecting tree occurrences.

<br>

